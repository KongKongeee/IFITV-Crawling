# -*- coding: utf-8 -*-
"""
Created on Fri May  9 17:03:39 2025

@author: Admin
"""

import os
import re
import time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import html
from urllib.parse import quote
from datetime import datetime, timedelta
import json
from dotenv import load_dotenv

load_dotenv()
    
# ë””ë ‰í† ë¦¬ ìƒì„± 
os.makedirs('./data_crawling_live', exist_ok=True)

'''
# ì„œë¸Œì¥ë¥´ ì¶”ì • í‚¤ì›Œë“œ (ìƒëµëœ ë¶€ë¶„ì€ ë™ì¼)
desc_keywords = {
    'í•´ì™¸ë“œë¼ë§ˆ': ['í•´ì™¸ë“œë¼ë§ˆ', 'ì™¸êµ­ ë“œë¼ë§ˆ', 'ì™¸í™”'],
    'ë¯¸êµ­ë“œë¼ë§ˆ': ['ë¯¸êµ­ ë“œë¼ë§ˆ', 'í• ë¦¬ìš°ë“œ'],
    'ì˜êµ­ë“œë¼ë§ˆ': ['ì˜êµ­ ë“œë¼ë§ˆ', 'BBC'],
    'ì¤‘êµ­ë“œë¼ë§ˆ': ['ì¤‘êµ­ ë“œë¼ë§ˆ', 'ì¤‘ë“œ'],
    'ì¼ë³¸ë“œë¼ë§ˆ': ['ì¼ë³¸ ë“œë¼ë§ˆ', 'ì¼ë“œ'],
    'ë¡œë§¨ìŠ¤': ['ì‚¬ë‘', 'ì—°ì• ', 'ë¡œë§¨ìŠ¤', 'ë©œë¡œ', 'ì²«ì‚¬ë‘'],
    'ì½”ë¯¸ë””': ['ì½”ë¯¸ë””', 'ìœ ì¾Œ', 'ì›ƒìŒ'],
    'íŒíƒ€ì§€': ['ë§ˆë²•', 'ì´ˆëŠ¥ë ¥', 'ì´ì„¸ê³„', 'íŒíƒ€ì§€'],
    'ë¬´í˜‘': ['ë¬´í˜‘', 'ê²€ìˆ ', 'ê°•í˜¸'],
    'ê³µí¬': ['ê³µí¬', 'í˜¸ëŸ¬', 'ê·€ì‹ '],
    'ë³µìˆ˜': ['ë³µìˆ˜', 'ë³´ë³µ'],
    'íœ´ë¨¼': ['ì¸ê°„ ë“œë¼ë§ˆ', 'ê°€ì¡±ì‚¬', 'ê°ë™', 'ê°€ì¡±', 'íœ´ë¨¼ ë“œë¼ë§ˆ'],
    'ë²”ì£„ ìŠ¤ë¦´ëŸ¬_ìˆ˜ì‚¬ê·¹': ['ìˆ˜ì‚¬', 'ìŠ¤ë¦´ëŸ¬', 'í˜•ì‚¬', 'ë²”ì£„'],
    'ì˜í•™': ['ë³‘ì›', 'ì˜ì‚¬', 'ì˜í•™', 'ì˜ë£Œ'],
    'ì›¹íˆ°_ì†Œì„¤ ì›ì‘': ['ì›¹íˆ° ì›ì‘', 'ì†Œì„¤ ì›ì‘', 'ë™ëª… ì›¹íˆ°'],
    'ì •ì¹˜_ê¶Œë ¥': ['ì •ì¹˜', 'ê¶Œë ¥', 'êµ­íšŒ'],
    'ë²•ì •': ['ë²•ì •', 'ë³€í˜¸ì‚¬', 'ì¬íŒ'],
    'ì²­ì¶˜': ['ì²­ì¶˜', 'ëŒ€í•™ìƒ', 'ìº í¼ìŠ¤'],
    'ì˜¤í”¼ìŠ¤ ë“œë¼ë§ˆ': ['ì§ì¥', 'íšŒì‚¬', 'ì˜¤í”¼ìŠ¤'],
    'ì‚¬ê·¹_ì‹œëŒ€ê·¹': ['ì¡°ì„ ', 'ì™•', 'ê¶', 'ì‚¬ê·¹', 'ì—­ì‚¬ê·¹'],
    'íƒ€ì„ìŠ¬ë¦½': ['íƒ€ì„ìŠ¬ë¦½', 'ì‹œê°„ ì—¬í–‰'],

    'ë²„ë¼ì´ì–´í‹°': ['ë²„ë¼ì´ì–´í‹°'],
    'ë‹¤íë©˜í„°ë¦¬': ['ë‹¤íë©˜í„°ë¦¬', 'ê¸°ë¡', 'ë¥´í¬'],
    'ì—¬í–‰': ['ì—¬í–‰', 'ê´€ê´‘', 'íˆ¬ì–´'],
    'ì¿¡ë°©/ë¨¹ë°©': ['ìš”ë¦¬', 'ì¿¡', 'ë¨¹ë°©', 'ë§›ì§‘'],
    'ì—°ì• ë¦¬ì–¼ë¦¬í‹°': ['ì—°ì•  ë¦¬ì–¼ë¦¬í‹°', 'ì†Œê°œíŒ…', 'ì—°ì•  í”„ë¡œê·¸ë¨'],
    'ê²Œì„': ['ê²Œì„', 'eìŠ¤í¬ì¸ '],
    'í† í¬ì‡¼': ['í† í¬ì‡¼', 'ì¸í„°ë·°', 'ëŒ€í™”'],
    'ì„œë°”ì´ë²Œ': ['ì„œë°”ì´ë²Œ', 'ì˜¤ë””ì…˜'],
    'ê´€ì°°ë¦¬ì–¼ë¦¬í‹°': ['ê´€ì°°', 'ë¦¬ì–¼ë¦¬í‹°', 'ì¼ìƒ ê³µê°œ'],
    'ìŠ¤í¬ì¸ ì˜ˆëŠ¥': ['ìŠ¤í¬ì¸  ì˜ˆëŠ¥', 'ìš´ë™ ì˜ˆëŠ¥'],
    'êµìœ¡ì˜ˆëŠ¥': ['êµìœ¡ ì˜ˆëŠ¥', 'ê³µë¶€ ì˜ˆëŠ¥', 'ì§€ì‹ ì „ë‹¬'],
    'íë§ì˜ˆëŠ¥': ['íë§ ì˜ˆëŠ¥', 'ìì—° ì˜ˆëŠ¥', 'íœ´ì‹'],
    'ì•„ì´ëŒ': ['ì•„ì´ëŒ', 'K-POP'],
    'ìŒì•…ì„œë°”ì´ë²Œ': ['ìŒì•… ì„œë°”ì´ë²Œ', 'ë³´ì»¬ ë°°í‹€'],
    'ìŒì•…ì˜ˆëŠ¥': ['ìŒì•… ì˜ˆëŠ¥', 'ë…¸ë˜ ì˜ˆëŠ¥', 'ìŠ¤íŠœë””ì˜¤'],
    'ê°€ì¡±ì˜ˆëŠ¥': ['ê°€ì¡± ì˜ˆëŠ¥', 'ê°€ì¡± ë¦¬ì–¼ë¦¬í‹°'],
    'ì½”ë¯¸ë””': ['ê°œê·¸', 'ì›ƒìŒ ì˜ˆëŠ¥'],
    'ë·°í‹°': ['ë·°í‹°', 'í™”ì¥', 'ë©”ì´í¬ì—…'],
    'ì• ë‹ˆë©€': ['ë™ë¬¼ í”„ë¡œê·¸ë¨', 'ë°˜ë ¤ë™ë¬¼'],
    'êµì–‘': ['êµì–‘ í”„ë¡œê·¸ë¨', 'ì§€ì‹', 'ì •ë³´ ì „ë‹¬', 'ë¼ì´í”„ìŠ¤íƒ€ì¼', 'ì˜í•™', 'ì§ˆë³‘', 'ê±´ê°•', 'ê³¼í•™', 'ì„¸ê³„ì‚¬', 'ì¸ë¬¸í•™'],

    'ë“œë¼ë§ˆ': ['ì˜í™” ë“œë¼ë§ˆ', 'ê°ë™ ì‹¤í™”'],
    'ë¡œë§¨ìŠ¤': ['ì˜í™” ë¡œë§¨ìŠ¤', 'ì‚¬ë‘ ì´ì•¼ê¸°'],
    'ì½”ë¯¸ë””': ['ì˜í™” ì½”ë¯¸ë””'],
    'ì• ë‹ˆë©”ì´ì…˜': ['ê·¹ì¥íŒ ì• ë‹ˆ', 'ì• ë‹ˆë©”ì´ì…˜ ì˜í™”'],
    'ìŠ¤ë¦´ëŸ¬': ['ìŠ¤ë¦´ëŸ¬ ì˜í™”', 'ê³µí¬ ìŠ¤ë¦´ëŸ¬'],
    'ë¯¸ìŠ¤í„°ë¦¬': ['ë¯¸ìŠ¤í„°ë¦¬ ì˜í™”', 'ì¶”ë¦¬'],
    'ëª¨í—˜': ['ëª¨í—˜ ì˜í™”', 'ì—¬ì •'],
    'ì•¡ì…˜': ['ì•¡ì…˜ ì˜í™”', 'ì „íˆ¬'],
    'íŒíƒ€ì§€ (ì˜í™”)': ['íŒíƒ€ì§€ ì˜í™”'],
    'SF': ['SF ì˜í™”', 'ê³¼í•™ íŒíƒ€ì§€'],
    'ê³µí¬': ['ê³µí¬ ì˜í™”'],
    'ë‹¤íë©˜í„°ë¦¬': ['ë‹¤í ì˜í™”'],

    'í‚¤ì¦ˆ': ['ì•„ì´ë“¤', 'ì–´ë¦°ì´', 'í‚¤ì¦ˆ', 'ì•„ë™ìš©', 'ë™ìš”']
}
'''

base_dir = os.path.dirname(__file__)
file_path = os.path.join(base_dir, 'desc_keywords.json')

with open(file_path, 'r', encoding='utf-8') as f:
    desc_keywords = json.load(f)

tmdb_genre_map = {
    28: 'ì•¡ì…˜',
    12: 'ëª¨í—˜',
    16: 'ì• ë‹ˆë©”ì´ì…˜',
    35: 'ì½”ë¯¸ë””',
    80: 'ìŠ¤ë¦´ëŸ¬',
    99: 'ë‹¤íë©˜í„°ë¦¬',
    18: 'ë“œë¼ë§ˆ',
    14: 'íŒíƒ€ì§€',
    27: 'ê³µí¬',
    9648: 'ë¯¸ìŠ¤í„°ë¦¬',
    10749: 'ë¡œë§¨ìŠ¤',
    878: 'SF',
    10770: 'ë“œë¼ë§ˆ',  # TV ì˜í™” â†’ ë“œë¼ë§ˆ ì²˜ë¦¬
    53: 'ìŠ¤ë¦´ëŸ¬',
    10752: 'ì•¡ì…˜',  # ì „ìŸ â†’ ì•¡ì…˜ ëŒ€ì²´
    37: 'ëª¨í—˜'  # ì„œë¶€ê·¹ â†’ ëª¨í—˜ìœ¼ë¡œ í†µí•©
}

allowed_subgenres_by_genre = {
    'ë“œë¼ë§ˆ': [
        'í•´ì™¸ë“œë¼ë§ˆ', 'ë¯¸êµ­ë“œë¼ë§ˆ', 'ì˜êµ­ë“œë¼ë§ˆ', 'ì¤‘êµ­ë“œë¼ë§ˆ', 'ì¼ë³¸ë“œë¼ë§ˆ',
        'ë¡œë§¨ìŠ¤', 'ì½”ë¯¸ë””', 'íŒíƒ€ì§€', 'ë¬´í˜‘', 'ê³µí¬', 'ë³µìˆ˜', 'íœ´ë¨¼', 'ë²”ì£„ ìŠ¤ë¦´ëŸ¬_ìˆ˜ì‚¬ê·¹',
        'ì˜í•™', 'ì›¹íˆ°_ì†Œì„¤ ì›ì‘', 'ì •ì¹˜_ê¶Œë ¥', 'ë²•ì •', 'ì²­ì¶˜', 'ì˜¤í”¼ìŠ¤ ë“œë¼ë§ˆ', 'ì‚¬ê·¹_ì‹œëŒ€ê·¹', 'íƒ€ì„ìŠ¬ë¦½'
    ],
    'ì˜ˆëŠ¥': [
        'ë²„ë¼ì´ì–´í‹°', 'ë‹¤íë©˜í„°ë¦¬', 'ì—¬í–‰', 'ì¿¡ë°©/ë¨¹ë°©', 'ì—°ì• ë¦¬ì–¼ë¦¬í‹°', 'ê²Œì„', 'í† í¬ì‡¼', 'ì„œë°”ì´ë²Œ',
        'ê´€ì°°ë¦¬ì–¼ë¦¬í‹°', 'ìŠ¤í¬ì¸ ì˜ˆëŠ¥', 'êµìœ¡ì˜ˆëŠ¥', 'íë§ì˜ˆëŠ¥', 'ì•„ì´ëŒ', 'ìŒì•…ì„œë°”ì´ë²Œ', 'ìŒì•…ì˜ˆëŠ¥',
        'ì½”ë¯¸ë””', 'ê°€ì¡±ì˜ˆëŠ¥', 'ë·°í‹°', 'ì• ë‹ˆë©€', 'êµì–‘'
    ],
    'ì˜í™”': [
        'ë“œë¼ë§ˆ', 'ë¡œë§¨ìŠ¤', 'ì½”ë¯¸ë””', 'ì• ë‹ˆë©”ì´ì…˜', 'ìŠ¤ë¦´ëŸ¬', 'ë¯¸ìŠ¤í„°ë¦¬',
        'ëª¨í—˜', 'ì•¡ì…˜', 'íŒíƒ€ì§€', 'SF', 'ê³µí¬', 'ë‹¤íë©˜í„°ë¦¬'
    ],
    'ì• ë‹ˆ': ['í‚¤ì¦ˆ'],
    'ë³´ë„': ['ë³´ë„']
}

def clean_subgenre_by_genre(original_genre, sub_genre):
    # ì˜ˆëŠ¥ì— ë“¤ì–´ê°€ë©´ ì•ˆ ë˜ëŠ” ë“œë¼ë§ˆìš© ì„œë¸Œì¥ë¥´
    if original_genre == 'ì˜ˆëŠ¥' and sub_genre in [
        'íœ´ë¨¼', 'ë¡œë§¨ìŠ¤', 'íŒíƒ€ì§€', 'ë¬´í˜‘', 'ê³µí¬', 'ë³µìˆ˜', 'ì˜í•™',
        'ì›¹íˆ°_ì†Œì„¤ ì›ì‘', 'ì •ì¹˜_ê¶Œë ¥', 'ë²•ì •', 'ì²­ì¶˜', 'ì˜¤í”¼ìŠ¤ ë“œë¼ë§ˆ',
        'ì‚¬ê·¹_ì‹œëŒ€ê·¹', 'íƒ€ì„ìŠ¬ë¦½', 'ë²”ì£„ ìŠ¤ë¦´ëŸ¬_ìˆ˜ì‚¬ê·¹'
    ]:
        return ''
    # ë“œë¼ë§ˆì— ì˜ˆëŠ¥ìš© ì„œë¸Œì¥ë¥´ê°€ ì˜ëª» ë“¤ì–´ì˜¨ ê²½ìš° ì œê±° (ì˜ˆ: ë·°í‹°)
    if original_genre == 'ë“œë¼ë§ˆ' and sub_genre not in allowed_subgenres_by_genre['ë“œë¼ë§ˆ']:
        return ''
    return sub_genre



# ì¥ë¥´ ë³€í™˜ ë§µ
genre_map = {'ì—°ì˜ˆ/ì˜¤ë½': 'ì˜ˆëŠ¥', 'ë‰´ìŠ¤/ì •ë³´': 'ë³´ë„', 'ë§Œí™”': 'ì• ë‹ˆ'}

def guess_subgenre_by_desc(desc):
    for subgenre, keywords in desc_keywords.items():
        for keyword in keywords:
            if keyword in desc:
                return subgenre
    return ''

def get_program_info_from_tmdb(title):
    def clean_title_for_tmdb(title):
    # ê´„í˜¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        title = re.sub(r'[\(\)\[\]ã€ˆã€‰â€œâ€"\'\:\-\|Â·,~!@#\$%\^&\*\+=]+', ' ', title)
        title = re.sub(r'\s+', ' ', title)  # ì—°ì† ê³µë°± ì •ë¦¬
        return title.strip()
    
    api_key = os.getenv("TMDB_API_KEY")
    image_base_url = "https://image.tmdb.org/t/p/w500"
    
    # ì˜ˆì™¸ ì²˜ë¦¬
    if title == "ìƒí™œì˜ ë°œê²¬":
        endpoints = [("tv", "name"), ("movie", "title")]
    else:
        endpoints = [("movie", "title"), ("tv", "name")]
    
    cleaned_title = clean_title_for_tmdb(title)

    for content_type, title_key in endpoints:
        try:
            # 1ì°¨ ê²€ìƒ‰
            search_url = f"https://api.themoviedb.org/3/search/{content_type}"
            params = {"api_key": api_key, "query": title, "language": "ko-KR"}
            search_res = requests.get(search_url, params=params)
            search_res.raise_for_status()
            results = search_res.json().get("results", [])

            if not results:
                continue  # ë‹¤ìŒ íƒ€ì…ìœ¼ë¡œ

            item = results[1] if title == 'ì¸ê°„ê·¹ì¥' and len(results) > 1 else results[0]
            content_id = item["id"]

            # 2ì°¨ ìƒì„¸ ì¡°íšŒ
            detail_url = f"https://api.themoviedb.org/3/{content_type}/{content_id}"
            detail_res = requests.get(detail_url, params={"api_key": api_key, "language": "ko-KR"})
            detail_res.raise_for_status()
            detail = detail_res.json()

            desc = detail.get("overview", "")
            poster_path = detail.get("poster_path")
            thumbnail = image_base_url + poster_path if poster_path else ''

            genre_ids = [g["id"] for g in detail.get("genres", [])]
            subgenres = list({tmdb_genre_map.get(gid) for gid in genre_ids if tmdb_genre_map.get(gid)})
            sub_genre = ', '.join(subgenres)
            


            return desc, thumbnail, sub_genre

        except Exception as e:
            print(f"[TMDb ì˜¤ë¥˜ - {content_type.upper()}] {title}: {e}")
            continue

    return '', '', ''


def get_program_info_from_tvmaze(title):
    try:
        search_url = f"https://api.tvmaze.com/search/shows?q={quote(title)}"
        res = requests.get(search_url, timeout=3)
        res.raise_for_status()
        results = res.json()
        if not results:
            return '', '', ''

        show = results[0]["show"]
        desc = html.unescape(show.get("summary", "").replace('<p>', '').replace('</p>', ''))
        thumbnail = show.get("image", {}).get("original", '') or show.get("image", {}).get("medium", '')
        genres = show.get("genres", [])
        sub_genre = ', '.join(genres)

        return desc.strip(), thumbnail, sub_genre

    except Exception as e:
        print(f"[TVmaze ì˜¤ë¥˜] {title}: {e}")
        return '', '', ''



def get_info_from_korean_wikipedia(program_name):
    import re
    import requests
    from bs4 import BeautifulSoup
    from urllib.parse import quote

    def clean_program_name_for_url(name):
        name = re.sub(r'\<.*?\>', '', name)
        name = re.sub(r'\[.*?\]', '', name)
        name = re.sub(r'\(.*?\)', '', name)
        name = re.sub(r'ã€ˆ.*?ã€‰', '', name)
        name = re.sub(r'[â€œâ€"\':\-]', '', name)
        name = re.sub(r'\s+', ' ', name).strip()
        return name

    def clean_text(text):
        text = re.sub(r'\([^)]*\)', '', text)
        text = re.sub(r'ã€ˆ.*?ã€‰', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        text = re.sub(r',\s*,', ',', text)
        return text

    def extract_subgenre(soup):
        a_tags = soup.select('table.infobox td a')
        for a in a_tags:
            txt = a.get_text(strip=True)
            for sub in desc_keywords.keys():
                if any(kw in txt for kw in desc_keywords[sub]):
                    return sub
        return ''

    # í´ë¦° ì²˜ë¦¬
    clean_name = clean_program_name_for_url(program_name)
    name_parts = clean_name.split()
    headers = {"User-Agent": "Mozilla/5.0"}

    # ì ì§„ì  ì¶•ì†Œ ê²€ìƒ‰
    for i in range(len(name_parts), 0, -1):
        try_name = ' '.join(name_parts[:i])
        wiki_url = f"https://ko.wikipedia.org/wiki/{quote(try_name)}"

        try:
            res = requests.get(wiki_url, headers=headers, timeout=3)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, 'html.parser')

            # ì„¤ëª… ì¶”ì¶œ
            desc = ''
            for p in soup.select('div.mw-parser-output > p'):
                text = p.get_text(strip=True)
                if len(text) > 50:
                    desc = clean_text(text)
                    break

            # ì„œë¸Œì¥ë¥´ ì¶”ì¶œ
            sub_genre = extract_subgenre(soup)

            return desc, sub_genre  # ì„±ê³µí•œ ê²½ìš° ì¦‰ì‹œ ë°˜í™˜

        except Exception:
            continue  # ì‹¤íŒ¨í•œ ê²½ìš° ë‹¤ìŒ ì´ë¦„ ì‹œë„

    return '', ''  # ì „ë¶€ ì‹¤íŒ¨ ì‹œ ë¹ˆ ê°’ ë°˜í™˜


def clean_name(text):
    # â‘  ê´„í˜¸ ë° íŠ¹ìˆ˜ ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì œê±°
    text = re.sub(r'\([^)]*\)', '', text)      # (ë‚´ìš©)
    text = re.sub(r'\[[^\]]*\]', '', text)     # [ë‚´ìš©]
    text = re.sub(r'ã€ˆ.*?ã€‰', '', text)         # ã€ˆë‚´ìš©ã€‰
    text = re.sub(r'\<.*?\>', '', text)        # <ë‚´ìš©>
    
    # â‘¡ ë°©ì†¡ ìƒíƒœ ê´€ë ¨ ë‹¨ì–´ ì œê±°
    text = re.sub(r'\b(ì¼ì¼ë“œë¼ë§ˆ|ì¬ë°©ì†¡|íŠ¹ë³„íŒ|ìŠ¤í˜ì…œ|ë³¸ë°©ì†¡|ë³¸|ì¬|íŠ¹ì§‘|ì¢…ì˜|ë§ˆì§€ë§‰íšŒ|ìµœì¢…í™”|HD|SD|NEW|ë‹¤ì‹œë³´ê¸°)\b', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\d+ë¶€', '', text)  # íšŒì°¨ ì •ë³´ ì œê±°
    
    # â‘¢ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    text = re.sub(r'[â€œâ€"\'\:\-\|Â·,~!@#\$%\^&\*\+=]+', ' ', text)  # ê¸°í˜¸ â†’ ê³µë°±
    text = re.sub(r'\s+', ' ', text)  # ì—°ì† ê³µë°± ì •ë¦¬
    
    # â‘£ í•œê¸€/ì˜ë¬¸ ì¡°í•© ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'([ê°€-í£])\s+([A-Za-z])', r'\1\2', text)
    text = re.sub(r'([A-Za-z])\s+([ê°€-í£])', r'\1\2', text)
    
    # â‘¤ ëì— ë‚¨ì€ ê´„í˜¸ ë“± ì œê±°
    text = text.strip("()[]ã€ˆã€‰ ")
    
    # â‘¥ ì „ì²´ ì •ë¦¬ í›„ ë°˜í™˜
    return text.strip()


def get_program_metadata(program_name, driver, original_genre):
    def get_info_from_web_search(name):
        genre_text = ''
        if name.strip() == "ê±¸ì–´ì„œ ì„¸ê³„ì†ìœ¼ë¡œ íŠ¸ë˜ë¸”í™€ë¦­":
            name = "ê±¸ì–´ì„œ ì„¸ê³„ì†ìœ¼ë¡œ"
        cleaned = clean_name(name)
        query = f"{cleaned} ì •ë³´"
        driver.get(f"https://search.naver.com/search.naver?query={quote(query)}")
        time.sleep(1.5)

        try:
            genre = driver.find_element(By.CSS_SELECTOR, "div.sub_title span").text.strip()
        except:
            genre = ''

        try:
            desc = driver.find_element(By.CSS_SELECTOR, "div.intro_box p").text.strip()
        except:
            desc = ''

        try:
            thumbnail = driver.find_element(
                By.CSS_SELECTOR,
                '#main_pack div[class*="_broadcast_button_scroller"] div.cm_content_wrap._broadcast_normal_total > div:nth-child(1) div.detail_info a img'
            ).get_attribute("src")
        except:
            thumbnail = ''
        
        try:
            genre_text = driver.find_element(
                By.CSS_SELECTOR,
                '#main_pack > div.sc_new.cs_common_module.case_empasis.color_13._au_movie_content_wrap > '
                'div.cm_content_wrap > div.cm_content_area._cm_content_area_info > div > '
                'div.detail_info > dl > div:nth-child(3) > dd'
            ).text.strip()
        except:
            pass

        return genre, desc, thumbnail, genre_text

    name = clean_name(program_name)

    # â‘  TMDb ì‹œë„
    tmdb_desc, tmdb_thumb, tmdb_sub = get_program_info_from_tmdb(name)

    # â‘¡ TMDb ì‹¤íŒ¨ ì‹œ TVmaze ë³´ì™„
    if not tmdb_desc and not tmdb_sub:
        tvmaze_desc, tvmaze_thumb, tvmaze_sub = get_program_info_from_tvmaze(name)
    else:
        tvmaze_desc, tvmaze_thumb, tvmaze_sub = '', '', ''

    # â‘¢ Wikipedia
    wiki_desc, wiki_sub = get_info_from_korean_wikipedia(name)

    # â‘£ NAVER Web
    web_genre, web_desc, web_thumb, genre_text = get_info_from_web_search(name)

    # ì„¤ëª… ìš°ì„ ìˆœìœ„
    desc = max([tmdb_desc, tvmaze_desc, wiki_desc, web_desc], key=lambda x: len(x or ''))
    
    

    # ì¸ë„¤ì¼ ìš°ì„ ìˆœìœ„
    thumbnail = tmdb_thumb or tvmaze_thumb or web_thumb or ''


    # ì„œë¸Œì¥ë¥´ ìš°ì„ ìˆœìœ„
    sub_genre = tmdb_sub or tvmaze_sub or wiki_sub or guess_subgenre_by_desc((genre_text or '') + " " + (desc or ''))

    # TMDb ì¥ë¥´ê°€ ì˜í™”ì¼ ê²½ìš° ì˜ˆëŠ¥ ì„œë¸Œì¥ë¥´ ì œê±°
    if original_genre == 'ì˜í™”':
        forbidden = ['ë²„ë¼ì´ì–´í‹°', 'ë‹¤íë©˜í„°ë¦¬', 'ì—¬í–‰', 'ì¿¡ë°©/ë¨¹ë°©', 'ì—°ì• ë¦¬ì–¼ë¦¬í‹°', 'ê²Œì„',
                     'í† í¬ì‡¼', 'ì„œë°”ì´ë²Œ', 'ê´€ì°°ë¦¬ì–¼ë¦¬í‹°', 'ìŠ¤í¬ì¸ ì˜ˆëŠ¥', 'êµìœ¡ì˜ˆëŠ¥', 'íë§ì˜ˆëŠ¥',
                     'ì•„ì´ëŒ', 'ìŒì•…ì„œë°”ì´ë²Œ', 'ìŒì•…ì˜ˆëŠ¥', 'ì½”ë¯¸ë””', 'ê°€ì¡±ì˜ˆëŠ¥', 'ë·°í‹°', 'ì• ë‹ˆë©€',
                     'êµì–‘', 'ë²”ì£„ ìŠ¤ë¦´ëŸ¬_ìˆ˜ì‚¬ê·¹']
        if sub_genre in forbidden:
            sub_genre = ''

    # ì¥ë¥´ ë³´ì •
    if web_genre == 'ì‹œì‚¬/êµì–‘':
        original_genre = 'ì˜ˆëŠ¥'
        sub_genre = 'êµì–‘'
    if sub_genre in ['ì–´ë¦°ì´', 'TVë§Œí™”', 'í‚¤ì¦ˆ']:
        original_genre = 'ì• ë‹ˆ'
        sub_genre = 'í‚¤ì¦ˆ'
    if original_genre == 'ìŠ¤í¬ì¸ ':
        original_genre = 'ì˜ˆëŠ¥'
        sub_genre = 'ìŠ¤í¬ì¸ ì˜ˆëŠ¥'
    if original_genre == 'ë³´ë„':
        sub_genre = 'ë³´ë„'
    if original_genre == 'ì• ë‹ˆ':
        sub_genre = 'í‚¤ì¦ˆ'
    if original_genre == 'ê³µì—°/ìŒì•…':
        original_genre = 'ì˜ˆëŠ¥'
        sub_genre = 'ìŒì•…ì„œë°”ì´ë²Œ'

    # ì¥ë¥´-ì„œë¸Œì¥ë¥´ ì •í•©ì„± ë³´ì •
    sub_genre = clean_subgenre_by_genre(original_genre, sub_genre)
    
    # í—ˆìš©ë˜ì§€ ì•Šì€ ì„œë¸Œì¥ë¥´ì´ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì¬ì¶”ë¡ 
    if not sub_genre or sub_genre not in allowed_subgenres_by_genre.get(original_genre, []):
        # ì˜ˆëŠ¥ì¸ë° TMDb ë“±ì—ì„œ "ëª¨í—˜"ì´ ë“¤ì–´ì˜¨ ê²½ìš° â†’ "ì—¬í–‰"ìœ¼ë¡œ ëŒ€ì²´
        if original_genre == 'ì˜ˆëŠ¥' and sub_genre == 'ëª¨í—˜':
            sub_genre = 'ì—¬í–‰'
        # ë“œë¼ë§ˆì¸ë° TMDb ë“±ì—ì„œ "ë·°í‹°" ë“¤ì–´ì˜¨ ê²½ìš° â†’ "íœ´ë¨¼"
        elif original_genre == 'ë“œë¼ë§ˆ' and sub_genre == 'ë·°í‹°':
            sub_genre = 'íœ´ë¨¼'
        else:
            # ì„¤ëª… ê¸°ë°˜ ì¬ì¶”ë¡  í›„ ì •í•©ì„± ê²€ì‚¬
            sub_genre = guess_subgenre_by_desc((genre_text or '') + " " + (desc or ''))
            sub_genre = clean_subgenre_by_genre(original_genre, sub_genre)

    desc  = re.sub(r'\s+', ' ', desc).strip()

    return original_genre, sub_genre, desc, thumbnail


def calculate_runtime(programs):
    new_list = []
    for i in range(len(programs)):
        current_time = datetime.strptime(programs[i][0], "%H:%M:%S")
        if i < len(programs) - 1:
            next_time = datetime.strptime(programs[i + 1][0], "%H:%M:%S")
            if next_time < current_time:
                next_time += timedelta(days=1)
            runtime = int((next_time - current_time).total_seconds() / 60)
        else:
            runtime = 60  # ë§ˆì§€ë§‰ í”„ë¡œê·¸ë¨ì€ 60ë¶„ìœ¼ë¡œ ê³ ì •
        new_list.append(programs[i][:4] + [runtime] + programs[i][4:])
    return new_list






# ì±„ë„ ë¦¬ìŠ¤íŠ¸ (ìƒëµëœ ë¶€ë¶„ ë™ì¼)
channel_list = [
    # ì „êµ­ ì§€ìƒíŒŒ
    'KBS1[9]', 'KBS2[7]', 'MBC[11]', 'SBS[5]',

    # ì¢…í¸ + ê³µì˜ + êµì–‘
    'JTBC[15]', 'MBN[16]', 'ì±„ë„A[18]', 'TVì¡°ì„ [19]',
    'EBS1[14]', 'EBS2[95]', 'OBS[26]',

    # ë“œë¼ë§ˆ/ì˜ˆëŠ¥/ì˜í™” ì „ë¬¸ ì±„ë„
    'tvN[3]', 'OCN[44]', 'ìŠ¤í¬ë¦°[46]', 'ì”¨ë„¤í”„[47]', 'OCN Movies2[51]',
    'ìºì¹˜ì˜¨1[52]', 'ìºì¹˜ì˜¨2[53]', 'ì±„ë„ì•¡ì…˜[54]',
    'ë“œë¼ë§ˆíë¸Œ[71]', 'ENA[72]', 'ENA DRAMA[73]',
    'KBS Story[74]', 'SBSí”ŒëŸ¬ìŠ¤[33]', 'MBCë“œë¼ë§ˆë„·[35]',

    # ì• ë‹ˆë©”ì´ì…˜/í‚¤ì¦ˆ ì±„ë„
    'íˆ¬ë‹ˆë²„ìŠ¤[324]', 'ì¹´íˆ°ë„¤íŠ¸ì›Œí¬[316]',
    'ì• ë‹ˆë°•ìŠ¤[327]', 'ì• ë‹ˆë§¥ìŠ¤[326]', 'ì–´ë¦°ì´TV[322]'
]

# í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •
options = Options()
# options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')

driver = webdriver.Chrome(options=options)
wait = WebDriverWait(driver, 10)

url = 'https://www.lguplus.com/iptv/channel-guide'
'''
driver.get(url)
driver.execute_script("document.body.style.zoom='50%'")
time.sleep(2)
'''
table_btn_xpath = '//a[contains(text(), "ì±„ë„ í¸ì„±í‘œ ì•ˆë‚´")]'
all_channel_btn_xpath = '//a[contains(text(), "ì „ì²´ì±„ë„")]'


# ì±„ë„ë³„ ë°˜ë³µ í¬ë¡¤ë§
for channel in channel_list:
    try:
        driver.get(url)
        driver.execute_script("document.body.style.zoom='50%'")
        time.sleep(1)
        
        wait.until(EC.element_to_be_clickable((By.XPATH, table_btn_xpath))).click()
        time.sleep(1)
        
        wait.until(EC.element_to_be_clickable((By.XPATH, all_channel_btn_xpath))).click()
        time.sleep(2)

        # ì±„ë„ íŒì—… ë‹¤ì‹œ ì—´ê¸°
        wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "a.c-btn-outline-2-s.open"))).click()
        time.sleep(1)

        # ì±„ë„ ë²„íŠ¼ í´ë¦­
        channel_xpath = f'//a[contains(text(), "{channel}")]'
        wait.until(EC.element_to_be_clickable((By.XPATH, channel_xpath))).click()
        time.sleep(2)
        
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        program_soup_list = soup.select('tr.point')

        program_list = []

        for item in program_soup_list:
            try:
                tds = item.select('td')
                time_text = tds[0].text.strip()
                name_parts = tds[1].text.split('\n')
                raw_name = name_parts[1].strip() if len(name_parts) > 1 else tds[1].text.strip()
                name = clean_name(raw_name)


                if name in ["ë°©ì†¡ ì‹œê°„ì´ ì•„ë‹™ë‹ˆë‹¤", "ë°©ì†¡ì‹œê°„ì´ ì•„ë‹™ë‹ˆë‹¤."]:
                    continue
                
                genre = genre_map.get(tds[2].text.strip(), tds[2].text.strip())
                
                result = get_program_metadata(name, driver, genre)
                if not result:
                    print(f"[ë©”íƒ€ë°ì´í„° ì—†ìŒ] {name}")
                    continue
                original_genre, sub_genre, desc, thumbnail = result
                program_list.append([time_text, name, original_genre, sub_genre, desc, thumbnail])
                
                time.sleep(0.2)
            except Exception as e:
                print(f"[í”„ë¡œê·¸ë¨ ì²˜ë¦¬ ì˜¤ë¥˜] {e}")
                continue
            
        # ëŸ°íƒ€ì„ ê³„ì‚° ì¶”ê°€ ì ìš©
        program_list = calculate_runtime(program_list)

        # ê²°ê³¼ ì €ì¥
        safe_name = re.sub(r'\s*(\[[^]]*\])', '', channel).strip()
        df = pd.DataFrame(program_list, columns = ['ë°©ì†¡ ì‹œê°„', 'í”„ë¡œê·¸ë¨ëª…', 'ì¥ë¥´', 'ì„œë¸Œì¥ë¥´','ìƒì˜ì‹œê°„(ë¶„)', 'ì„¤ëª…', 'ì¸ë„¤ì¼'])
        
        # ğŸ”§ ìŒë”°ì˜´í‘œ ì œê±° + ì‰¼í‘œ â†’ íƒ­ìœ¼ë¡œ ë³€í™˜
        df['ì„œë¸Œì¥ë¥´'] = df['ì„œë¸Œì¥ë¥´'].apply(
            lambda x: x.replace('"', '') if isinstance(x, str) else x
        )
        
        # ì €ì¥
        df.to_csv(f'./data_crawling_live/{safe_name}_program_list.csv', index=False, encoding='utf-8-sig')

        print(f"[ì™„ë£Œ] {channel} â†’ ì €ì¥ ì™„ë£Œ")
        time.sleep(1)

    except Exception as e:
        print(f"[ì±„ë„ ì˜¤ë¥˜] {channel} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        continue

# ë“œë¼ì´ë²„ ì¢…ë£Œ
driver.quit()
print("[ì „ì²´ ì™„ë£Œ] ëª¨ë“  ì±„ë„ í¬ë¡¤ë§ ì¢…ë£Œ")